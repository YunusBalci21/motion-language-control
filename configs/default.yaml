# configs/default.yaml
# Motion-Language Control - Stable Training Configuration
# Fixed hyperparameters to prevent KL divergence explosion

experiment:
  name: "motion_language_control"
  description: "Humanoid motion-language control with stable PPO"
  version: "4.0"
  authors: ["Yunus Emre Balci"]
  institution: "SDU"

environment:
  id: "Humanoid-v4"
  name: "Humanoid-v4"
  num_envs: 1
  max_episode_steps: 1000

  # Stability wrapper settings
  action_scale: 0.4          # Lower = more stable movements
  action_smoothing: 0.7      # Higher = smoother actions
  target_height: 1.25        # Target standing height
  min_height: 0.8            # Termination threshold
  initial_noise: 0.01        # Initial state randomization

ppo:
  # Learning rate - CRITICAL: single consistent value
  learning_rate: 1.0e-4      # Conservative LR to prevent KL explosion
  lr_schedule: "linear"      # Decay over training

  # GAE parameters
  gamma: 0.99
  gae_lambda: 0.95

  # PPO clipping - tighter for stability
  clip_range: 0.1            # Was 0.2, tighter prevents large updates
  clip_range_vf: null        # No value clipping

  # Batch settings
  n_steps: 2048              # Steps per rollout
  batch_size: 2048           # Same as n_steps for single env
  minibatch_size: 128        # Larger minibatches = more stable
  n_epochs: 5                # Fewer epochs = less overfitting per batch

  # Loss coefficients
  value_coef: 0.5
  entropy_coef: 0.005        # Lower entropy = less random exploration
  max_grad_norm: 0.5

  # Early stopping
  target_kl: 0.03            # Stop epoch if KL exceeds this
  max_kl: 0.05               # Hard limit for KL divergence

training:
  total_timesteps: 2000000

  # Logging & saving
  log_interval: 10           # Episodes between logs
  eval_interval: 50000       # Steps between evaluations
  save_interval: 50000       # Steps between checkpoints
  eval_episodes: 5

  # Directories
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  video_dir: "videos"

network:
  hidden_sizes: [256, 256]
  activation: "tanh"
  ortho_init: true
  log_std_init: -0.5         # Initial action std (exp(-0.5) â‰ˆ 0.6)

rewards:
  use_motion_language: true
  stability_weight: 0.5      # Balance: 0.5 stability, 0.5 task
  motion_reward_scale: 3.0   # Scale for motion-language reward

  # Curriculum - gradually reduce stability focus
  curriculum:
    - phase: 1
      instruction: "stand still"
      steps: 200000
      stability_weight: 0.7
    - phase: 2
      instruction: "walk forward slowly"
      steps: 300000
      stability_weight: 0.5
    - phase: 3
      instruction: "walk forward"
      steps: 500000
      stability_weight: 0.4
    - phase: 4
      instruction: "walk forward"
      steps: 500000
      stability_weight: 0.3

instructions:
  basic:
    - "stand still"
    - "walk forward"
    - "walk forward slowly"
    - "walk backward"
    - "turn left"
    - "turn right"

evaluation:
  n_episodes: 5
  deterministic: true
  record_video: true
  video_length: 500

hardware:
  device: "auto"             # auto/cuda/cpu
  mixed_precision: false

reproducibility:
  seed: 42
  deterministic: false