# configs/default.yaml (UPDATE - reduce stability focus initially)
experiment:
  name: "direct_motion_language_learning"
  description: "Enhanced motion-language control with forward movement focus"
  version: "3.1"
  authors: ["Yunus Emre Balci"]
  institution: "SDU"

algo: ppo

environment:
  id: "Humanoid-v4"
  name: "Humanoid-v4"
  alternatives: ["HalfCheetah-v4", "Ant-v4", "Walker2d-v4"]
  num_envs: 8
  max_episode_steps: 1000
  max_steps: 1000
  render_mode: null

training:
  total_timesteps_per_instruction: 200000  # Increased
  language_reward_weights: [0.5, 0.6, 0.7, 0.8]  # Start higher
  n_parallel_envs: 4
  use_multiprocessing: false
  eval_freq: 10000
  checkpoint_freq: 5000

ppo:
  lr: 5.0e-4  # Increased learning rate
  learning_rate: 5.0e-4
  gamma: 0.99
  lam: 0.95
  gae_lambda: 0.95
  clip_range: 0.2
  clip_ratio: 0.2
  steps_per_update: 8192
  n_steps: 2048
  batch_size: 64
  minibatches: 32
  epochs: 10
  n_epochs: 10
  vf_coef: 0.5
  ent_coef: 0.01  # Increased for exploration
  max_grad_norm: 0.5
  norm_adv: true

instructions:
  basic:
    - "walk forward"
    - "walk forward slowly"
    - "walk forward quickly"
    - "walk backward"
    - "turn left"
    - "turn right"
    - "stop moving"

  intermediate:
    - "walk forward steadily"
    - "walk forward without falling"
    - "run forward"
    - "turn left while walking"
    - "turn right while walking"

  stability_focused:
    - "walk stably"
    - "walk without falling"
    - "maintain balance while moving"

evaluation:
  n_eval_episodes: 5
  eval_deterministic: true
  cross_evaluation: true
  benchmark_against_clip: true
  compute_similarity_metrics: true
  track_computation_times: true
  save_episode_videos: true
  similarity_threshold_good: 0.5  # Lowered
  similarity_threshold_excellent: 0.7  # Lowered
  reward_threshold_success: 80.0  # Lowered

motion_gpt:
  config_path: null
  checkpoint_path: null
  language_model: "t5-small"
  motion_dim: 263
  max_sequence_length: 196
  codebook_size: 512
  latent_dim: 256
  temporal_aggregation: "weighted_recent"
  reward_smoothing: 0.8
  success_threshold: 0.5
  motion_history_length: 20
  motion_normalization: true

hierarchical:
  use_hierarchical_structure: false
  low_level_freq: 10
  high_level_freq: 1
  discover_skills: false
  skill_embedding_dim: 64
  n_discovered_skills: 16

logging:
  tensorboard_log: "./logs/"
  wandb_project: null
  log_level: "INFO"
  save_model_every_n_steps: 10000
  track_metrics:
    - "motion_language_similarity"
    - "language_reward"
    - "original_env_reward"
    - "episode_length"
    - "computation_time"
    - "motion_variance"

experimental:
  use_progress_bonus: true
  use_instruction_specific_rewards: true
  stability_bonus_weight: 0.2  # Reduced
  use_motion_augmentation: false
  augmentation_noise_std: 0.01
  adaptive_curriculum: false
  curriculum_success_threshold: 0.6
  use_visual_features: false
  use_audio_instructions: false

hardware:
  device: "auto"
  mixed_precision: false
  compile_model: false
  gradient_accumulation_steps: 1
  max_memory_usage_gb: 8.0

reproducibility:
  seed: 42
  deterministic_training: false
  save_random_state: true
  save_training_curves: true
  save_model_architecture: true
  save_hyperparameters: true
  generate_performance_plots: true

baselines:
  clip_based_reward: true
  pixel_rendering_time: 7
  clip_encoding_time: 15
  random_policy: false
  pretrained_locomotion: false
  vision_based_reward: false