
\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}

\title{Motion-Language Control for Humanoid Robots\\with Stability Enhancements}

\author{
\IEEEauthorblockN{Your Name}
\IEEEauthorblockA{Syddansk Universitet\\
Odense, Denmark\\
Email: youremail@sdu.dk}
}
git
\begin{document}

\maketitle

\begin{abstract}
We present a direct motion-language learning approach for training humanoid robots to follow natural language instructions. Our method integrates MotionGPT tokenization with reinforcement learning and incorporates novel stability-focused reward shaping. Through progressive curriculum learning across four phases, we achieve 100\% task success rates for walking behaviors while maintaining robust stability.
\end{abstract}

\section{Introduction}
Natural language interfaces for robot control offer intuitive human-robot interaction...

\section{Methodology}

\subsection{Motion-Language Alignment}
We use MotionGPT to extract motion features...

\subsection{Reward Function}
Our total reward combines multiple components:
\begin{equation}
R_{total} = (1-\alpha)R_{env} + \alpha(R_{lang} + R_{progress} + R_{stab} - R_{energy})
\end{equation}

\section{Results}

\begin{table}[h]
\centering
\caption{Quantitative Results Across Training Phases}
\begin{tabular}{lcccc}
\toprule
Phase & Reward & Length & Success & Falls \\
\midrule
Slow Walk & 3630 & 218 & 100\% & 13.4 \\
Normal Walk & 3489 & 205 & 100\% & 1.5 \\
Endurance & 3418 & 200 & 100\% & 2.7 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
We demonstrated successful language-conditioned humanoid locomotion...

\end{document}
